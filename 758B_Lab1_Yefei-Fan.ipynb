{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy package as np\n",
    "import numpy as np\n",
    "# Import Dense from keras.layers\n",
    "from keras.layers import Dense\n",
    "# Import Sequential from keras.models\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# Fix random seed as 7 for reproducibility\n",
    "np.random.seed(7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pima indians diabetes dataset\n",
    "dataset = np.loadtxt('/Users/fyf/Desktop/758B-lab1/pima-indians-diabetes.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data set into input X and output Y\n",
    "# Assign the first eight input variables to X\n",
    "X = dataset[:, 0:8]\n",
    "# Assign the last output class variable to varaible Y\n",
    "Y = dataset[:, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an empty sequential network\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create first layer with 12 neurons, relu activation function and 8-dimension input\n",
    "model.add(Dense(12, activation = 'relu', input_dim = 8))\n",
    "# Create second layer with 8 neurons and relu activation function\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "# Create output layer with 1 neuron and sigmoid activation function\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model using binary cross entropy to calculate loss, adam gradient descent to optimize the metric accuracy\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 1s 929us/step - loss: 3.7104 - acc: 0.5990\n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s 203us/step - loss: 0.9359 - acc: 0.5938\n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s 162us/step - loss: 0.7472 - acc: 0.6406\n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s 140us/step - loss: 0.7115 - acc: 0.6576\n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s 133us/step - loss: 0.6823 - acc: 0.6745\n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s 129us/step - loss: 0.6511 - acc: 0.6849\n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.6495 - acc: 0.6771\n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s 127us/step - loss: 0.6369 - acc: 0.6836\n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s 136us/step - loss: 0.6246 - acc: 0.6979\n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.6293 - acc: 0.6771\n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.6472 - acc: 0.6745\n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s 129us/step - loss: 0.6377 - acc: 0.6732\n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s 127us/step - loss: 0.6253 - acc: 0.6758\n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.6165 - acc: 0.7031\n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s 133us/step - loss: 0.6016 - acc: 0.6979\n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s 127us/step - loss: 0.5877 - acc: 0.7044\n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s 144us/step - loss: 0.5843 - acc: 0.6992\n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s 178us/step - loss: 0.6007 - acc: 0.6849\n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s 167us/step - loss: 0.5802 - acc: 0.7096\n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s 140us/step - loss: 0.5804 - acc: 0.7161\n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.5684 - acc: 0.7148\n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.5821 - acc: 0.6966\n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.5740 - acc: 0.7135\n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s 160us/step - loss: 0.5677 - acc: 0.7305\n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s 162us/step - loss: 0.5579 - acc: 0.7344\n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s 177us/step - loss: 0.5704 - acc: 0.7031\n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s 129us/step - loss: 0.5555 - acc: 0.7240\n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s 123us/step - loss: 0.5555 - acc: 0.7305\n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s 159us/step - loss: 0.5741 - acc: 0.7148 0s - loss: 0.5821 - acc: 0.711\n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s 180us/step - loss: 0.5611 - acc: 0.7227\n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s 148us/step - loss: 0.5694 - acc: 0.7187\n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s 139us/step - loss: 0.5629 - acc: 0.7174\n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s 138us/step - loss: 0.5525 - acc: 0.7161\n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s 158us/step - loss: 0.5508 - acc: 0.7370\n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s 166us/step - loss: 0.5496 - acc: 0.7201\n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s 168us/step - loss: 0.5651 - acc: 0.7070\n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.5332 - acc: 0.7396\n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s 129us/step - loss: 0.5408 - acc: 0.7253\n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5461 - acc: 0.7253\n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.5450 - acc: 0.7227\n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s 153us/step - loss: 0.5438 - acc: 0.7344\n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s 170us/step - loss: 0.5380 - acc: 0.7409\n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s 161us/step - loss: 0.5302 - acc: 0.7500\n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s 136us/step - loss: 0.5344 - acc: 0.7422\n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s 135us/step - loss: 0.5323 - acc: 0.7526\n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s 142us/step - loss: 0.5281 - acc: 0.7526\n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s 162us/step - loss: 0.5319 - acc: 0.7396\n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s 147us/step - loss: 0.5339 - acc: 0.7435\n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s 145us/step - loss: 0.5339 - acc: 0.7422\n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s 143us/step - loss: 0.5266 - acc: 0.7396\n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s 133us/step - loss: 0.5275 - acc: 0.7487\n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s 141us/step - loss: 0.5323 - acc: 0.7461\n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s 175us/step - loss: 0.5394 - acc: 0.7409\n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s 172us/step - loss: 0.5385 - acc: 0.7253\n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s 181us/step - loss: 0.5221 - acc: 0.7552\n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s 159us/step - loss: 0.5287 - acc: 0.7448\n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s 151us/step - loss: 0.5321 - acc: 0.7344\n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s 130us/step - loss: 0.5217 - acc: 0.7500\n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5133 - acc: 0.7617\n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s 148us/step - loss: 0.5356 - acc: 0.7370\n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s 163us/step - loss: 0.5275 - acc: 0.7357\n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s 156us/step - loss: 0.5168 - acc: 0.7565\n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.5447 - acc: 0.7318\n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5305 - acc: 0.7396\n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s 153us/step - loss: 0.5212 - acc: 0.7461\n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s 159us/step - loss: 0.5076 - acc: 0.7500\n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s 160us/step - loss: 0.5163 - acc: 0.7357\n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s 139us/step - loss: 0.5153 - acc: 0.7526\n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s 143us/step - loss: 0.5149 - acc: 0.7513\n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s 160us/step - loss: 0.5392 - acc: 0.7240\n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s 158us/step - loss: 0.5184 - acc: 0.7370\n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s 153us/step - loss: 0.5180 - acc: 0.7448\n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s 146us/step - loss: 0.5163 - acc: 0.7487\n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s 160us/step - loss: 0.5114 - acc: 0.7578\n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s 242us/step - loss: 0.5110 - acc: 0.7578\n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s 162us/step - loss: 0.5127 - acc: 0.7513\n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.5154 - acc: 0.7656\n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.5124 - acc: 0.7539\n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s 134us/step - loss: 0.5132 - acc: 0.7461\n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s 145us/step - loss: 0.5094 - acc: 0.7617\n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s 158us/step - loss: 0.5041 - acc: 0.7786\n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s 135us/step - loss: 0.5027 - acc: 0.7604\n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s 156us/step - loss: 0.5022 - acc: 0.7669\n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s 152us/step - loss: 0.4979 - acc: 0.7565\n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s 129us/step - loss: 0.5043 - acc: 0.7526\n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s 147us/step - loss: 0.5045 - acc: 0.7526\n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s 152us/step - loss: 0.4975 - acc: 0.7617\n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s 166us/step - loss: 0.4998 - acc: 0.7669\n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s 147us/step - loss: 0.5030 - acc: 0.7747\n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s 157us/step - loss: 0.5082 - acc: 0.7565\n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s 256us/step - loss: 0.5013 - acc: 0.7617\n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s 174us/step - loss: 0.5062 - acc: 0.7487\n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s 155us/step - loss: 0.4992 - acc: 0.7669\n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s 150us/step - loss: 0.4982 - acc: 0.7656\n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s 151us/step - loss: 0.5032 - acc: 0.7500\n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s 157us/step - loss: 0.4916 - acc: 0.7695\n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s 144us/step - loss: 0.4980 - acc: 0.7682\n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s 146us/step - loss: 0.4905 - acc: 0.7591\n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s 142us/step - loss: 0.4909 - acc: 0.7669\n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s 139us/step - loss: 0.4846 - acc: 0.7812\n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s 148us/step - loss: 0.4903 - acc: 0.7747\n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s 145us/step - loss: 0.4979 - acc: 0.7669\n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s 149us/step - loss: 0.4982 - acc: 0.7552\n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s 151us/step - loss: 0.4921 - acc: 0.7956\n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s 164us/step - loss: 0.5321 - acc: 0.7474\n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s 144us/step - loss: 0.4900 - acc: 0.7799\n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s 139us/step - loss: 0.4899 - acc: 0.7708\n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s 149us/step - loss: 0.4959 - acc: 0.7708\n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s 151us/step - loss: 0.4873 - acc: 0.7656\n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s 149us/step - loss: 0.4891 - acc: 0.7721\n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.4840 - acc: 0.7799\n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s 158us/step - loss: 0.4920 - acc: 0.7786\n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s 146us/step - loss: 0.4964 - acc: 0.7591\n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s 160us/step - loss: 0.4922 - acc: 0.7656\n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s 149us/step - loss: 0.4907 - acc: 0.7708\n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s 146us/step - loss: 0.4900 - acc: 0.7734\n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s 148us/step - loss: 0.4906 - acc: 0.7578\n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s 146us/step - loss: 0.4880 - acc: 0.7826\n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.4822 - acc: 0.7721\n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s 147us/step - loss: 0.4940 - acc: 0.7747\n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s 147us/step - loss: 0.4918 - acc: 0.7812\n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s 156us/step - loss: 0.4835 - acc: 0.7878\n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s 143us/step - loss: 0.4810 - acc: 0.7721\n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.4839 - acc: 0.7826\n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.4858 - acc: 0.7799\n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.4791 - acc: 0.7734\n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.4890 - acc: 0.7708\n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.4707 - acc: 0.7760\n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s 175us/step - loss: 0.4800 - acc: 0.7786 0s - loss: 0.4798 - acc: 0.768\n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s 193us/step - loss: 0.4729 - acc: 0.7865\n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s 134us/step - loss: 0.4813 - acc: 0.7695\n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.4832 - acc: 0.7747\n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s 151us/step - loss: 0.4817 - acc: 0.7695\n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s 150us/step - loss: 0.4849 - acc: 0.7695\n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s 129us/step - loss: 0.4767 - acc: 0.7721\n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.4744 - acc: 0.7786\n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.4662 - acc: 0.7812\n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s 123us/step - loss: 0.4793 - acc: 0.7826\n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s 133us/step - loss: 0.4638 - acc: 0.7799\n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.4812 - acc: 0.7839\n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s 136us/step - loss: 0.4727 - acc: 0.7865\n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s 166us/step - loss: 0.4826 - acc: 0.7708\n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s 189us/step - loss: 0.4744 - acc: 0.7721\n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s 152us/step - loss: 0.4721 - acc: 0.7826\n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s 127us/step - loss: 0.4891 - acc: 0.7578\n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.4883 - acc: 0.7682\n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s 123us/step - loss: 0.4820 - acc: 0.7799\n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.4716 - acc: 0.7721\n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.4753 - acc: 0.7630\n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.4772 - acc: 0.7799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb24da6fd0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model with X and Y and set batch size as 10 and epochs as 150\n",
    "model.fit(X, Y, epochs = 150, batch_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 212us/step\n",
      "\n",
      "acc: 77.99%\n"
     ]
    }
   ],
   "source": [
    "# Use the same data to evaluate model and return accuracy score 77.99%\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\"%(model.metrics_names[1],scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import string\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ItemID  Sentiment                                      SentimentText\n",
      "0       1          0                       is so sad for my APL frie...\n",
      "1       2          0                     I missed the New Moon trail...\n",
      "2       3          1                            omg its already 7:30 :O\n",
      "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
      "4       5          0           i think mi bf is cheating on me!!!   ...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('/Users/fyf/Desktop/758B-lab1/train_validation.csv', sep = ',', encoding = 'latin1')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove unwanted text patterns from the tweets\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@user, punctuations, numbers and pecial characters don't help in differentiating different kinds of tweets.\n",
    "Most of the smaller words do not add much value. So, we will try to remove them from our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Twitter Handles (@user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ItemID  Sentiment                                      SentimentText  \\\n",
      "0       1          0                       is so sad for my APL frie...   \n",
      "1       2          0                     I missed the New Moon trail...   \n",
      "2       3          1                            omg its already 7:30 :O   \n",
      "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
      "4       5          0           i think mi bf is cheating on me!!!   ...   \n",
      "\n",
      "                                         clean_tweet  \n",
      "0                       is so sad for my APL frie...  \n",
      "1                     I missed the New Moon trail...  \n",
      "2                            omg its already 7:30 :O  \n",
      "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
      "4           i think mi bf is cheating on me!!!   ...  \n"
     ]
    }
   ],
   "source": [
    "data['clean_tweet'] = np.vectorize(remove_pattern)(data['SentimentText'], \"@[\\w]*\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Punctuations, Numbers and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_tweet'] = data['clean_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>omg its already       O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>Omgaga  Im sooo  im gunna CRy  I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>i think mi bf is cheating on me      ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText  \\\n",
       "0       1          0                       is so sad for my APL frie...   \n",
       "1       2          0                     I missed the New Moon trail...   \n",
       "2       3          1                            omg its already 7:30 :O   \n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4       5          0           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already       O  \n",
       "3               Omgaga  Im sooo  im gunna CRy  I ...  \n",
       "4           i think mi bf is cheating on me      ...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Short Words (all the words having length 2 or less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_tweet'] = data['clean_tweet'].apply(lambda x:' '.join([w for w in x.split() if len(w) > 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>sad for APL friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>missed the New Moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>omg its already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>Omgaga sooo gunna CRy been this dentist since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>think cheating</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText  \\\n",
       "0       1          0                       is so sad for my APL frie...   \n",
       "1       2          0                     I missed the New Moon trail...   \n",
       "2       3          1                            omg its already 7:30 :O   \n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4       5          0           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0                                 sad for APL friend  \n",
       "1                        missed the New Moon trailer  \n",
       "2                                    omg its already  \n",
       "3  Omgaga sooo gunna CRy been this dentist since ...  \n",
       "4                                     think cheating  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization -- convert sentence to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              [sad, for, APL, friend]\n",
       "1                    [missed, the, New, Moon, trailer]\n",
       "2                                  [omg, its, already]\n",
       "3    [Omgaga, sooo, gunna, CRy, been, this, dentist...\n",
       "4                                    [think, cheating]\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet = data['clean_tweet'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can reduce different terms of same word to their root word, such as 'loving', 'loves', 'loved' to ‘love’, then we can reduce the total number of unique words in our data without losing a significant amount of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              [sad, for, apl, friend]\n",
       "1                      [miss, the, new, moon, trailer]\n",
       "2                                   [omg, it, alreadi]\n",
       "3    [omgaga, sooo, gunna, cri, been, thi, dentist,...\n",
       "4                                       [think, cheat]\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stitch these tokens back together\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "data['clean_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Words Features: represent text into numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Set the parameter max_features = 8000 to select only top 8000 terms ordered by term frequency \n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=8000, stop_words='english')\n",
    "\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(data['clean_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<99989x8000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 547559 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79991, 8000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from keras.layers import Dropout\n",
    "\n",
    "import numpy as np \n",
    "np.random.seed(123)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "# splitting data into training set(80%) and validation set(20%)\n",
    "Xtrain, Xvalid, ytrain, yvalid = train_test_split(bow, data['Sentiment'], \n",
    "                                                          random_state=42, test_size=0.2)\n",
    "print(Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<79991x8000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 437574 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an empty sequential network\n",
    "model = Sequential()\n",
    "# Create first layer with 100 neurons, relu activation function and 8000-dimension input\n",
    "model.add(Dense(100, input_dim = 8000, activation = 'relu'))\n",
    "# Set each neuron has 50% probability to dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Create second layer with 100 neurons and relu activation function\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "# Set each neuron has 60% probability to dropout\n",
    "model.add(Dropout(0.6))\n",
    "# Create third layer with 50 neurons and relu activation function\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "# Set each neuron has 70% probability to dropout \n",
    "model.add(Dropout(0.7))\n",
    "# Create output layer with 1 neuron and sigmoid activation function\n",
    "model.add(Dense(1,activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model using binary cross entropy to calculate loss, adam gradient descent to optimize the metric accuracy\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "79991/79991 [==============================] - 10s 131us/step - loss: 0.6903 - acc: 0.5359\n",
      "Epoch 2/8\n",
      "79991/79991 [==============================] - 9s 108us/step - loss: 0.6782 - acc: 0.5679\n",
      "Epoch 3/8\n",
      "79991/79991 [==============================] - 9s 109us/step - loss: 0.6591 - acc: 0.5750\n",
      "Epoch 4/8\n",
      "79991/79991 [==============================] - 9s 111us/step - loss: 0.6254 - acc: 0.6333\n",
      "Epoch 5/8\n",
      "79991/79991 [==============================] - 9s 108us/step - loss: 0.5836 - acc: 0.7099\n",
      "Epoch 6/8\n",
      "79991/79991 [==============================] - 9s 111us/step - loss: 0.5483 - acc: 0.7426\n",
      "Epoch 7/8\n",
      "79991/79991 [==============================] - 9s 108us/step - loss: 0.5223 - acc: 0.7594\n",
      "Epoch 8/8\n",
      "79991/79991 [==============================] - 9s 108us/step - loss: 0.4998 - acc: 0.7746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb19077e48>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model with X and Y in training set and set batch size as 8 and epochs as 10000\n",
    "model.fit(Xtrain, ytrain, epochs = 8, batch_size = 10000 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19998/19998 [==============================] - 3s 138us/step\n",
      "Total loss on Testing Set:  0.516385626996776\n",
      "Accuracy on Testiong Set:  74.79747975155144\n"
     ]
    }
   ],
   "source": [
    "# Use data in validation set to evaluate model and return accuracy score 74.79%\n",
    "score = model.evaluate(Xvalid, yvalid)\n",
    "print('Total loss on Testing Set: ', score[0])\n",
    "print('Accuracy on Testiong Set: ', score[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
